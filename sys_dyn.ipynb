{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTeUFibvMGjEpYAyGnp+rW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dvoils/neural-network-experiments/blob/main/sys_dyn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Associative Memory and Hopfield Networks\n",
        "* Hopfield, 1982, Neural networks and physical systems with emergent collective computational abilities,\n",
        "https://www.pnas.org/doi/epdf/10.1073/pnas.79.8.2554\n",
        "\n",
        "## Spin Glass\n",
        "* Amit et al, 1985, Storing Infinite Numbers of Patterns in a Spin-Glass Model of Neural Networks,\n",
        "https://gwern.net/doc/ai/nn/1985-amit.pdf\n",
        "* Hidetoshi Nishimori, 2001, Statistical pysicics of spin glass and information processing,\n",
        "https://t2r2.star.titech.ac.jp/rrws/file/CTT100922878/ATD100000413/\n",
        "\n",
        "## Oscillatory Neural Networks (ONNs) as Generalized Attractor Systems\n",
        "\n",
        "* **Porod et al. (2023)** train oscillator states using **backpropagation through time**—a nontrivial dynamical approach.\n",
        "\n",
        "### Concepts\n",
        "\n",
        "* Local or global synchronization defines associative recall.\n",
        "* Unlike Hopfield nets (binary or continuous states), ONNs use **phase, amplitude, or frequency** as state variables.\n",
        "* Oscillators define richer dynamics (limit cycles, phase locking, chaos).\n",
        "* Attractor becomes not a point but a **trajectory** or **phase-locked manifold**.\n",
        "* More expressive “memory” than point attractors.\n",
        "\n",
        "\n",
        "## 3. CNNs and OCNNs (Roska & Chua) as Dynamical Substrates\n",
        "\n",
        "* **CNN Universal Machine**: A 2D grid of locally coupled cells (like pixels), governed by ODEs.\n",
        "\n",
        "### Concepts\n",
        "* Each cell is a dynamical system with neighbors affecting it, enabling fast, parallel processing.\n",
        "* Oscillatory modes allow encoding time-varying phenomena.\n",
        "\n",
        "### Applications\n",
        "\n",
        "* Image processing (edge detection, motion tracking)\n",
        "* Analog/digital hybrid computation\n",
        "* Field-programmable analog arrays (FPAAs)\n",
        "\n",
        "## 4. FPGA Implementations and Resource-Aware Architectures\n",
        "\n",
        "* **Szalkai & Porod** and **Ohi et al. (2023)** show ONNs achieving Hopfield-like associative memory with fewer resources.\n",
        "\n",
        "### Concepts\n",
        "* Hebbian vs. Storkey rules tested.\n",
        "* Implementation challenges: clock precision, jitter, quantization of phase/amplitude.\n",
        "\n",
        "\n",
        "## 5. Dynamical Systems View: Fixed Points, Limit Cycles, and Chaos\n",
        "\n",
        "* Hopfield → fixed point attractors.\n",
        "* ONNs → phase synchrony or limit cycle locking.\n",
        "* Hopf oscillators (Wang et al., 2024) bring bifurcation theory directly into network dynamics.\n",
        "* Dynamics of learning: Stability, bifurcations, training through dynamical transitions.\n",
        "\n"
      ],
      "metadata": {
        "id": "MPXAqiD41zG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These papers have a strong relevance to understanding the intersection of **oscillatory computing**, **neural networks**, **dynamical systems**, and **AI**:\n",
        "\n",
        "## 1. **Learning algorithms for oscillatory neural networks as associative memory for pattern recognition** (Jiménez et al., 2023)\n",
        "\n",
        "* **Why it stands out**: Demonstrates how to adapt classic Hopfield-style learning (Hebbian, Storkey) to ONNs, implements *on-chip continual learning*, and benchmarks ONNs against HNNs in pattern recognition—providing a rich look into dynamics, learning, and hardware synergy ([Frontiers][1]).\n",
        "\n",
        "## 2. **Design of Oscillatory Neural Networks by Machine Learning** (Rudner, Porod & Csaba, 2023)\n",
        "\n",
        "* **Why it stands out**: Introduces **BPTT** to train ONNs at the circuit level, optimizing physical parameters. Offers insight into tuning dynamic coupling to achieve BNN-like performance in hardware—critical for bridging analog/digital implementations with AI-trained networks .\n",
        "\n",
        "## 3. **Oscillatory neural network as hetero-associative memory for image edge detection** (Abernot, Gil & Todri‑Sanial, 2022)\n",
        "\n",
        "* **Why it stands out**: Applies ONNs to real-world **edge detection**—a classic AI/vision task—using **hetero-associative memory**, focusing on image-based pattern transformation in dynamic systems that mimic CNN filter behavior ([arXiv][2]).\n",
        "\n",
        "## 4. **Dynamical associative memory based on an oscillatory neural network** (Ng & Feng, 2001)\n",
        "\n",
        "* **Why it stands out**: One of the earliest formal models exploring **discrete-time oscillatory neural networks (DTONN)**, with analysis of **spatio-temporal chaos** and attractor dynamics. Offers foundational theory on **dynamical systems behavior** in associative memory ([De Gruyter Brill][3]).\n",
        "\n"
      ],
      "metadata": {
        "id": "pfi6zDIPSKUG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s0Z2nrGYEA7D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}