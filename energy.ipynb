{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPizh4f26MOr/LgbHyr3fVi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dvoils/neural-network-experiments/blob/main/energy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Hopfield proposed that certain computational properties useful to organisms and computers can emerge **collectively** from large assemblies of simple interacting units (neurons). Instead of requiring complex circuitry, emergent computation arises spontaneously, analogous to physical systems such as magnetic domains or fluid vortices. This paper presents a model that exhibits **content-addressable memory**, error correction, generalization, and categorization---all emerging from dynamics rather than programmed instruction.\n",
        "\n",
        "\n",
        "## Content-Addressable Memory and Dynamics\n",
        "\n",
        "Let a memory be represented as a point $\\mathbf{X} \\in \\mathbb{R}^N$. In certain physical systems (e.g., Ising models), dynamics defined by gradient descent in energy space drive states toward stable attractors:\n",
        "\n",
        "$$\n",
        "\\frac{d\\mathbf{X}}{dt} = -\\nabla E(\\mathbf{X})\n",
        "$$\n",
        "\n",
        "This system acts as a **content-addressable memory** if every partial or noisy input state $\\mathbf{X}' \\approx \\mathbf{X}_a$ flows toward a stable point $\\mathbf{X}_a$. Hopfield demonstrates that such dynamics can recover full memories from fragments.\n",
        "\n",
        "## The Hopfield Model\n",
        "\n",
        "Each of the $N$ neurons is binary:\n",
        "\n",
        "$V_i \\in \\{0, 1\\} \\quad \\text{or equivalently} \\quad s_i = 2V_i - 1 \\in \\{-1, +1\\}$\n",
        "\n",
        "Neurons update asynchronously using the rule:\n",
        "\n",
        "$$\n",
        "V_i \\leftarrow\n",
        "\\begin{cases}\n",
        "1 & \\text{if } \\sum_j T_{ij} V_j > U_i \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "where $T_{ij}$ is the synaptic strength, and $U_i$ is the threshold (often taken to be 0).\n",
        "\n",
        "\n",
        "## Hebbian Learning Rule\n",
        "\n",
        "To store a set of binary patterns $\\{\\mathbf{V}^s\\}_{s=1}^n$, Hopfield applies the Hebbian learning rule:\n",
        "\n",
        "$$\n",
        "T_{ij} = \\sum_{s=1}^{n} (2V_i^s - 1)(2V_j^s - 1), \\quad T_{ii} = 0\n",
        "$$\n",
        "\n",
        "This rule causes each stored pattern $\\mathbf{V}^s$ to become a local minimum (attractor) in energy space:\n",
        "\n",
        "$$\n",
        "E = -\\frac{1}{2} \\sum_{i \\neq j} T_{ij} V_i V_j\n",
        "$$\n",
        "\n",
        "Updating any neuron causes the energy to decrease:\n",
        "\n",
        "$$\n",
        "\\Delta E = -\\Delta V_i \\sum_j T_{ij} V_j\n",
        "$$\n",
        "\n",
        "Thus, asynchronous updates guarantee convergence to a stable state.\n",
        "\n",
        "## Capacity and Error Correction\n",
        "\n",
        "* For $N$ neurons, the network can stably store about $0.15N$ random patterns before retrieval degrades.\n",
        "* Noise in the system is modeled as Gaussian, leading to error probability:\n",
        "\n",
        "$$\n",
        "P = \\frac{1}{\\sqrt{2\\pi}} \\int_x^\\infty e^{-t^2/2} dt\n",
        "$$\n",
        "\n",
        "* Simulations confirm recall is accurate for low pattern count and degrades as $n$ approaches $0.15N$.\n",
        "\n",
        "## Categorization and Familiarity\n",
        "\n",
        "* **Generalization**: The system categorizes ambiguous inputs by converging to the closest memory.\n",
        "* **Familiarity**: High activation rates during convergence can indicate whether a pattern is familiar.\n",
        "* **Categorical recall**: Close patterns may collapse into a shared attractor (useful for pattern completion).\n",
        "\n",
        "## Extensions and Properties\n",
        "\n",
        "* **Clipped Weights**: Even if $T_{ij} \\in \\{-1, 0, +1\\}$, performance only degrades slightly.\n",
        "* **Asymmetry**: Even non-symmetric $T_{ij} \\neq T_{ji}$ can yield metastable attractors.\n",
        "* **Forgetting**: Saturating synaptic strength (e.g., $T_{ij} \\in [-3, 3]$) introduces natural forgetting.\n",
        "* **Sequence Recall**: Adding asymmetric terms can allow short sequences $V^1 \\to V^2 \\to V^3 \\to \\dots$.\n",
        "\n",
        "## Biological Plausibility\n",
        "\n",
        "* Real neurons exhibit firing rates that approximate binary thresholds.\n",
        "* Hebbian learning ($\\Delta T_{ij} \\propto V_i V_j$) is biologically plausible.\n",
        "* Delay and stochasticity are modeled via asynchronous updates.\n",
        "\n",
        "\n",
        "## Conclusions\n",
        "\n",
        "Hopfield demonstrates that associative memory and pattern completion can emerge as collective properties of simple neuron-like elements. These results suggest:\n",
        "\n",
        "1. Neural computation does not require complex sequential logic.\n",
        "2. Distributed systems can perform robust parallel computation.\n",
        "3. The brain may exploit such physical dynamics for memory, recognition, and decision-making.\n",
        "4. Hardware implementations (e.g., neuromorphic chips) could benefit from these ideas.\n",
        "\n",
        "**Key Concepts**: attractor dynamics, energy minimization, Hebbian learning, content-addressable memory, neural computation, error correction, categorization.\n",
        "\n",
        "**Citations**: J.J. Hopfield, PNAS, Vol. 79, pp. 2554â€“2558, 1982.\n"
      ],
      "metadata": {
        "id": "lSE5-W4ZLBY-"
      }
    }
  ]
}