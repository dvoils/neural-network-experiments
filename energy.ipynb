{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1hY6KjIipG36/b84YHxeY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dvoils/neural-network-experiments/blob/main/energy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Hopfield proposed that certain computational properties useful to organisms and computers can emerge **collectively** from large assemblies of simple interacting units (neurons). Instead of requiring complex circuitry, emergent computation arises spontaneously, analogous to physical systems such as magnetic domains or fluid vortices. This paper presents a model that exhibits **content-addressable memory**, error correction, generalization, and categorization---all emerging from dynamics rather than programmed instruction.\n",
        "\n",
        "\n",
        "## Content-Addressable Memory and Dynamics\n",
        "\n",
        "Let a memory be represented as a point $\\mathbf{X} \\in \\mathbb{R}^N$. In certain physical systems (e.g., Ising models), dynamics defined by gradient descent in energy space drive states toward stable attractors:\n",
        "\n",
        "$$\n",
        "\\frac{d\\mathbf{X}}{dt} = -\\nabla E(\\mathbf{X})\n",
        "$$\n",
        "\n",
        "This system acts as a **content-addressable memory** if every partial or noisy input state $\\mathbf{X}' \\approx \\mathbf{X}_a$ flows toward a stable point $\\mathbf{X}_a$. Hopfield demonstrates that such dynamics can recover full memories from fragments.\n",
        "\n",
        "## The Hopfield Model\n",
        "\n",
        "Each of the $N$ neurons is binary:\n",
        "\n",
        "$V_i \\in \\{0, 1\\} \\quad \\text{or equivalently} \\quad s_i = 2V_i - 1 \\in \\{-1, +1\\}$\n",
        "\n",
        "Neurons update asynchronously using the rule:\n",
        "\n",
        "$$\n",
        "V_i \\leftarrow\n",
        "\\begin{cases}\n",
        "1 & \\text{if } \\sum_j T_{ij} V_j > U_i \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "where $T_{ij}$ is the synaptic strength, and $U_i$ is the threshold (often taken to be 0).\n",
        "\n",
        "\n",
        "## Hebbian Learning Rule\n",
        "\n",
        "To store a set of binary patterns $\\{\\mathbf{V}^s\\}_{s=1}^n$, Hopfield applies the Hebbian learning rule:\n",
        "\n",
        "$$\n",
        "T_{ij} = \\sum_{s=1}^{n} (2V_i^s - 1)(2V_j^s - 1), \\quad T_{ii} = 0\n",
        "$$\n",
        "\n",
        "This rule causes each stored pattern $\\mathbf{V}^s$ to become a local minimum (attractor) in energy space:\n",
        "\n",
        "$$\n",
        "E = -\\frac{1}{2} \\sum_{i \\neq j} T_{ij} V_i V_j\n",
        "$$\n",
        "\n",
        "Updating any neuron causes the energy to decrease:\n",
        "\n",
        "$$\n",
        "\\Delta E = -\\Delta V_i \\sum_j T_{ij} V_j\n",
        "$$\n",
        "\n",
        "Thus, asynchronous updates guarantee convergence to a stable state.\n",
        "\n",
        "## Capacity and Error Correction\n",
        "\n",
        "* For $N$ neurons, the network can stably store about $0.15N$ random patterns before retrieval degrades.\n",
        "* Noise in the system is modeled as Gaussian, leading to error probability:\n",
        "\n",
        "$$\n",
        "P = \\frac{1}{\\sqrt{2\\pi}} \\int_x^\\infty e^{-t^2/2} dt\n",
        "$$\n",
        "\n",
        "* Simulations confirm recall is accurate for low pattern count and degrades as $n$ approaches $0.15N$.\n",
        "\n",
        "## Categorization and Familiarity\n",
        "\n",
        "* **Generalization**: The system categorizes ambiguous inputs by converging to the closest memory.\n",
        "* **Familiarity**: High activation rates during convergence can indicate whether a pattern is familiar.\n",
        "* **Categorical recall**: Close patterns may collapse into a shared attractor (useful for pattern completion).\n",
        "\n",
        "## Extensions and Properties\n",
        "\n",
        "* **Clipped Weights**: Even if $T_{ij} \\in \\{-1, 0, +1\\}$, performance only degrades slightly.\n",
        "* **Asymmetry**: Even non-symmetric $T_{ij} \\neq T_{ji}$ can yield metastable attractors.\n",
        "* **Forgetting**: Saturating synaptic strength (e.g., $T_{ij} \\in [-3, 3]$) introduces natural forgetting.\n",
        "* **Sequence Recall**: Adding asymmetric terms can allow short sequences $V^1 \\to V^2 \\to V^3 \\to \\dots$.\n",
        "\n",
        "## Biological Plausibility\n",
        "\n",
        "* Real neurons exhibit firing rates that approximate binary thresholds.\n",
        "* Hebbian learning ($\\Delta T_{ij} \\propto V_i V_j$) is biologically plausible.\n",
        "* Delay and stochasticity are modeled via asynchronous updates.\n",
        "\n",
        "\n",
        "## Conclusions\n",
        "\n",
        "Hopfield demonstrates that associative memory and pattern completion can emerge as collective properties of simple neuron-like elements. These results suggest:\n",
        "\n",
        "1. Neural computation does not require complex sequential logic.\n",
        "2. Distributed systems can perform robust parallel computation.\n",
        "3. The brain may exploit such physical dynamics for memory, recognition, and decision-making.\n",
        "4. Hardware implementations (e.g., neuromorphic chips) could benefit from these ideas.\n",
        "\n",
        "**Key Concepts**: attractor dynamics, energy minimization, Hebbian learning, content-addressable memory, neural computation, error correction, categorization.\n",
        "\n",
        "**Citations**: J.J. Hopfield, PNAS, Vol. 79, pp. 2554–2558, 1982.\n"
      ],
      "metadata": {
        "id": "lSE5-W4ZLBY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1.1 Representing Neural Memory States\n",
        "\n",
        "In statistical models of associative memory, a network of $N$ binary neurons can store and retrieve discrete memory patterns by evolving toward specific stable configurations. Each neuron $i \\in \\{1, 2, \\ldots, N\\}$ assumes a value $S_i \\in \\{-1, +1\\}$, representing either a quiescent or active state, respectively. The complete state of the system at a given moment is defined by the collection of these binary variables, which form a configuration vector in $N$-dimensional binary space.\n",
        "\n",
        "To formally define the storage of memory patterns, we adopt the notation:\n",
        "\n",
        "$$\n",
        "| \\alpha, t \\rangle = | S_1^\\alpha S_2^\\alpha, \\ldots, S_N^\\alpha \\rangle\n",
        "\\tag{1.1}\n",
        "$$\n",
        "\n",
        "Here, $\\alpha \\in \\{1, 2, \\ldots, p\\}$ indexes the memory pattern, with $p$ denoting the total number of patterns stored in the network. The superscript $\\alpha$ on each $S_i^\\alpha$ identifies the value of neuron $i$ in the $\\alpha$-th pattern. The ket notation $| \\alpha, t \\rangle$ is borrowed from quantum mechanics and serves to emphasize that the system configuration is treated as a discrete state vector in a high-dimensional space of neuron activations.\n",
        "\n",
        "Each memory pattern $\\alpha$ corresponds to a unique point in this configuration space, and is defined by a fixed binary string:\n",
        "\n",
        "$$\n",
        "\\boldsymbol{S}^\\alpha = (S_1^\\alpha, S_2^\\alpha, \\ldots, S_N^\\alpha)\n",
        "$$\n",
        "\n",
        "These pattern vectors are embedded into the network via synaptic interactions and serve as **attractors** in the system’s energy landscape. During retrieval, a noisy or partial input pattern evolves over time toward the nearest stored pattern—typically by minimizing an appropriately defined energy function.\n",
        "\n",
        "Although the time index $t$ appears in $| \\alpha, t \\rangle$, in this context it does not imply temporal evolution of the pattern itself. Rather, it indicates that at time $t$, the system is in—or converging toward—the stored pattern $\\alpha$. In subsequent sections of the model, this interpretation becomes central to the retrieval dynamics governed by spin-glass-inspired energy minimization.\n",
        "\n",
        "Equation (1.1) encapsulates the foundational idea of attractor memory: each $\\boldsymbol{S}^\\alpha$ represents a learned configuration that the neural network is capable of recalling via its intrinsic dynamics. The collection of these stored states defines the computational repertoire of the network.\n"
      ],
      "metadata": {
        "id": "8NBZ422cQk2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 Synaptic Input (Local Potential)\n",
        "\n",
        "To determine whether a neuron will flip its state, the model assigns each neuron a **local synaptic potential** $V_i$.  In the notation of *Amit, Gutfreund & Sompolinsky* this potential is defined by\n",
        "\n",
        "$$\n",
        "V_i \\;=\\; \\sum_{j} J_{i,j}\\,\\bigl(S_j + 1\\bigr)\n",
        "\\tag{1.2}\n",
        "$$\n",
        "\n",
        "Where,\n",
        "\n",
        "* **$V_i$** – the total input (sometimes called the *local field* or *membrane potential*) experienced by neuron $i$.\n",
        "* **$J_{i,j}$** – the fixed synaptic coupling from neuron $j$ to neuron $i$.  These couplings were set during learning (later formalised by the Hebbian rule in Eq. 1.5).\n",
        "* **$S_j\\in\\{-1,+1\\}$** – the present state of neuron $j$.  A value $+1$ stands for “active/firing,” while $-1$ stands for “inactive/silent.”\n",
        "* **Shift term $(S_j+1)$** – by adding $1$ the authors map the binary alphabet $\\{-1,+1\\}$ to $\\{0,2\\}$.\n",
        "\n",
        "  * If $S_j = -1$ (silent) then $S_j+1 = 0$: neuron $j$ contributes **nothing** to $V_i$.\n",
        "  * If $S_j = +1$ (active) then $S_j+1 = 2$: neuron $j$ contributes **$2J_{i,j}$** to $V_i$.\n",
        "\n",
        "This “rectified” form ensures that only active presynaptic neurons inject current; silent neurons are effectively ignored.  The factor 2 can later be absorbed into the choice of threshold.\n",
        "\n",
        "## Neural‐network interpretation\n",
        "\n",
        "Equation (1.2) states that neuron $i$ integrates weighted inputs from every other neuron that is currently firing.  Once the potential $V_i$ is computed, a deterministic update rule (introduced immediately after Eq. 1.2 in the paper) flips the neuron according to whether $V_i$ exceeds a threshold $U_i$:\n",
        "\n",
        "$$\n",
        "S_i(t+1) =\n",
        "\\begin{cases}\n",
        "+1, & V_i(t) \\;>\\; U_i,\\\\[4pt]\n",
        "-1, & V_i(t) \\;\\le\\; U_i.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "When all thresholds are set to $U_i=0$ and the constant factor 2 is absorbed into $J_{i,j}$, this reduces to the familiar “sign” rule often written $S_i(t+1)=\\operatorname{sgn}\\bigl(\\sum_j J_{i,j}S_j(t)\\bigr)$.\n",
        "\n",
        "## Statistical‐physics perspective\n",
        "\n",
        "From the spin‑glass viewpoint, the shift $(S_j+1)$ is a convenience: it isolates the **interaction** between spins (captured by $J_{i,j}S_j$) from a constant “background field.”  Whether one works with $\\{0,2\\}$ or $\\{-1,+1\\}$ is immaterial to the thermodynamics as long as the Hamiltonian is defined consistently.  What matters is that the nervous system’s dynamics still amount to **energy descent** toward stored attractors.\n",
        "\n",
        "## Key takeaway\n",
        "\n",
        "Equation (1.2) formalises how each neuron translates the collective activity of the network into a single scalar $V_i$.  By adding the offset $+1$, the authors guarantee that **only active presynaptic neurons contribute positive current**, simplifying later analytical work without changing the essential physics of attractor formation.\n",
        "\n"
      ],
      "metadata": {
        "id": "GNE5N-Ftagri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "$$"
      ],
      "metadata": {
        "id": "_o5FtEj-enQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 Neuron Update Condition: Threshold-Based Dynamics\n",
        "\n",
        "In the spin-glass neural network framework, each neuron evaluates whether to flip its state by comparing its **local input** to a **threshold**. This comparison is captured in Equation (1.3):\n",
        "\n",
        "$$\n",
        "S_i h_i = S_i (V_i - U_i) > 0\n",
        "\\tag{1.3}\n",
        "$$\n",
        "\n",
        "Where,\n",
        "\n",
        "* $S_i \\in \\{-1, +1\\}$: the current state of neuron $i$.\n",
        "* $V_i$: the **local synaptic input** or potential to neuron $i$, defined in Equation (1.2) as:\n",
        "\n",
        "  $$\n",
        "  V_i = \\sum_j J_{i,j}(S_j + 1)\n",
        "  $$\n",
        "* $U_i$: the **threshold** for neuron $i$, determining the input level required for it to become active.\n",
        "* $h_i = V_i - U_i$: the **net driving input** to neuron $i$.\n",
        "\n",
        "Equation (1.3) expresses a condition for *stability* of the neuron’s current state:\n",
        "\n",
        "> A neuron is said to be stable if its state $S_i$ is **aligned** with its net input $h_i$.\n",
        "\n",
        "This is enforced by checking whether the product:\n",
        "\n",
        "$$\n",
        "S_i h_i = S_i (V_i - U_i)\n",
        "$$\n",
        "\n",
        "is **positive**. If it is, then the neuron is **already in the correct state** (aligned with its input). If it’s negative, the neuron is **misaligned** and will flip in the next update.\n",
        "\n",
        "## Neural Dynamics\n",
        "\n",
        "The rule is **deterministic**: each neuron flips only if doing so **reduces the system’s energy** (as we'll see in Eq. 1.4). So, the update proceeds as:\n",
        "\n",
        "* If $S_i h_i > 0$: neuron $i$ is **stable**, and no change occurs.\n",
        "* If $S_i h_i < 0$: neuron $i$ is **unstable**, and will flip in the next iteration:\n",
        "\n",
        "$$\n",
        "S_i \\rightarrow -S_i\n",
        "$$\n",
        "\n",
        "This is analogous to **gradient descent** on the energy landscape: neurons flip only when doing so lowers the system’s energy.\n",
        "\n",
        "## Physical Analogy\n",
        "\n",
        "From the spin-glass point of view, this condition reflects whether the **spin $S_i$** is aligned with the **local effective magnetic field** $h_i$. Stability corresponds to spins minimizing the energy contribution of their local field.\n"
      ],
      "metadata": {
        "id": "oIOc_rV4e8iT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 Energy Function of the Network: The Hamiltonian\n",
        "\n",
        "The evolution of the neural network can be understood as a process of **energy minimization**. Stable states (memories) correspond to local minima of an energy landscape. This energy is formalized by the **Hamiltonian** $H$, given in Equation (1.4):\n",
        "\n",
        "$$\n",
        "H = -\\sum_i h_i S_i = -\\frac{1}{2} \\sum_{i,j} J_{ij} S_i S_j\n",
        "\\tag{1.4}\n",
        "$$\n",
        "\n",
        "Where,\n",
        "\n",
        "* $H$: The **Hamiltonian**, or total energy, of the neural network.\n",
        "* $S_i \\in \\{-1, +1\\}$: The state of neuron (spin) $i$.\n",
        "* $h_i$: The **molecular field** (or local effective field) acting on neuron $i$, defined by:\n",
        "\n",
        "$$\n",
        "h_i = \\sum_j J_{ij} S_j\n",
        "$$\n",
        "\n",
        "  — which corresponds to $V_i - U_i$ under the assumption $U_i = 0$.\n",
        "* $J_{ij}$: The **synaptic coupling** from neuron $j$ to neuron $i$, assumed symmetric: $J_{ij} = J_{ji}$.\n",
        "\n",
        "## Interpretation\n",
        "\n",
        "This Hamiltonian encodes the system’s tendency to evolve toward configurations where the spins are **aligned with their molecular fields**. The negative sign ensures that the system minimizes energy when spins and fields are aligned:\n",
        "\n",
        "$$\n",
        "S_i = \\text{sign}(h_i)\n",
        "$$\n",
        "\n",
        "In the first form,\n",
        "\n",
        "$$\n",
        "H = -\\sum_i h_i S_i\n",
        "$$\n",
        "\n",
        "we interpret the energy as a sum over all neurons, each contributing based on the alignment between its state $S_i$ and the local field $h_i$.\n",
        "\n",
        "In the second, equivalent form,\n",
        "\n",
        "$$\n",
        "H = -\\frac{1}{2} \\sum_{i,j} J_{ij} S_i S_j\n",
        "$$\n",
        "\n",
        "the Hamiltonian is expressed directly in terms of pairwise spin interactions. The factor of $\\frac{1}{2}$ corrects for double-counting each symmetric pair $(i,j)$.\n",
        "\n",
        "## Dynamical Implication\n",
        "\n",
        "Because the update rule (Eq. 1.3) flips a spin only if it **decreases the energy**, the entire system evolves by **sequential energy descent**:\n",
        "\n",
        "* Each individual spin flip that satisfies $S_i h_i < 0$ leads to $\\Delta H < 0$.\n",
        "* The dynamics are **dissipative** — the system moves toward **local minima** of $H$, which are the **stable states or attractors**.\n",
        "\n",
        "## Physics Analogy\n",
        "\n",
        "In a spin glass, spins interact via both ferromagnetic and antiferromagnetic couplings, leading to **frustration** — where not all pairwise interactions can be simultaneously satisfied. This creates a **rugged energy landscape** with many local minima. In the neural model, these minima correspond to **memorized patterns**.\n",
        "\n",
        "This equation formally aligns the neural network with the **Sherrington-Kirkpatrick (SK) model** of spin glasses, except that here, the couplings $J_{ij}$ are **structured** — designed through learning — rather than randomly drawn.\n",
        "\n"
      ],
      "metadata": {
        "id": "0N3_UDvet9xJ"
      }
    }
  ]
}